{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMw3UYrgW3PsKrjVwHir4E+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimanirobbi/wk-7-ai/blob/main/Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 1: Biased Hiring Tool"
      ],
      "metadata": {
        "id": "IJczgK4ZjGiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Source of Bias:**\n",
        "\n",
        "The primary source was **biased training data**. The model was trained on a decade of resumes from a male-dominated tech industry, leading it to associate male candidates with being more \"suitable.\"\n",
        "\n",
        "## **Three Proposed Fixes:**\n",
        "\n",
        "1. **Debias the Training Data:** Curate a more balanced dataset that includes an equal representation of qualified male and female candidates. Use techniques like re-sampling or re-weighting the data.\n",
        "\n",
        "2. **Remove Proxy Variables:** Identify and remove features that act as proxies for gender (e.g., the name of a women's college, participation in all-female sports teams, or certain gendered phrasing).\n",
        "\n",
        "3. **Implement Fairness Constraints:** Use in-processing techniques (like those in AIF360) to add constraints to the model during training that enforce demographic parity or equal opportunity.\n",
        "\n",
        "**Metrics to Evaluate Fairness:**\n",
        "\n",
        "- **Disparate Impact Ratio:** Measures the ratio of selection rates between the unprivileged (women) and privileged (men) groups. A value close to 1.0 indicates fairness.\n",
        "\n",
        "- **Equal Opportunity Difference:** (True Positive Rate for women) - (True Positive Rate for men). A value close to 0 indicates that qualified candidates from both groups have the same chance of being selected."
      ],
      "metadata": {
        "id": "X43yb85hkr7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 2: Facial Recognition in Policing"
      ],
      "metadata": {
        "id": "ck-mDuQkkwHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ethical Risks:**\n",
        "\n",
        "- **Wrongful Arrests and Convictions:** Misidentification can lead to innocent people being detained, with severe personal, professional, and psychological consequences.\n",
        "\n",
        "- **Erosion of Privacy and Mass Surveillance:** Pervasive use can create a surveillance state, chilling freedom of assembly and expression, especially in marginalized communities.\n",
        "\n",
        "- **Reinforcement of Societal Bias:** Higher error rates for minorities can exacerbate existing racial disparities in the criminal justice system.\n",
        "\n",
        "## **Policies for Responsible Deployment:**\n",
        "\n",
        "- **Mandatory Third-Party Audits:** Require independent audits for accuracy and bias across different demographics before deployment.\n",
        "\n",
        "- **Strict Use-Case Limitations:** Ban its use as the sole evidence for an arrest. It should only be used to generate leads, which must be corroborated by traditional investigative methods.\n",
        "\n",
        "- **Legislative Oversight and Transparency:** Pass laws requiring public reporting on the system's usage, accuracy rates, and demographic impact."
      ],
      "metadata": {
        "id": "dzBt0ttbk7gl"
      }
    }
  ]
}